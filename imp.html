<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>IT304 Project</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
<header>
        <h1>Virtual Agent</h1>
</header>
    <nav>
		<div class="navbar">
 <ul>
    <li><a href="index.html">Introduction</a></li>    
    <li><a href="app.html">Applications</a></li>
    <li><a href="imp.html">Implications</a></li>
</ul>

	
    <main>
        <section class="homepage">
            <h2>Implications</h2>
			  
	
</div>

<main id="main-content">
        <section class="intro">

<h1>Social implications</h1>
<p>Reduce human interaction: relying on VA too much takes away the social interaction with a person, losing out in the connecting/build rapport.</p>
<ul>Ex doctor with patient</ul>

<h1>Political/ Socil Concerns</h1>
<p>Privacy and Data Security: integration of VAs in healthcare, or in different fields, systems involve changing sensitive data, bringing in concerns of data privacy, security, and potential misuse.</p>
<p>In terms of a privacy concern, currently AI algorithms are being used in social media by, allegedly, listening in people’s conversation and searching through our search history to “tailor” the content we see to the user. There is also evidence of this claim when instances of virtual agent respond to the user even though the voice activation phrase wasn’t spoken and the command or question wasn’t directed towards it.</p>

<p>Equity and access: Not all fields and facilities have access to VAs to use</p>
<p>Misplaced trust: over reliance and trust VAs, assuming that they are always correct or helpful.</p>
<p>In the context of medical treatment, the agent could miss a detail and misdiagnose a patient. The agent is only as knowledgeable and experienced as it is limited to the depth of the data that it is programmed with.</p>

<h1>Built-in harms/risks</h1>
<p>Mistakes in communication: VAs can misunderstand/mishear the user’s words, leading to the wrong intended results.</p>
<p>Too much trust in the technology: too much reliance</p>
<ul>Ex. doctors/hospital over rely, leading to lower quality of care</ul>

<p>No proven benefit: if the technology isn’t usable or helpful, then they are a waste of time/resources and could cause harm by replacing better solutions
</p>

<h2>Ethical Lens</2h>

<h1>Utilitarian (usefulness)</h1>
<p>Interpretation: VAs can be used to increase efficiency, reduce costs, and improve access in different fields (healthcare, customer service, education), ethical</p>
<p>Social Impact:</p>
<ul>Positive: For areas with marginalized populations, the use can be seen as a positive.</ul>
<ul>Negative: f the VA begin to replace the jobs of workers</ul>
<p>Ex: chat bots that offer mental support can help reduce stress and sucicide rates, but can replace the interaction/experience of going to a human therapist.</p>

<h1>Deontology (duty-based)</h1>
<p>Interpretation: Using VAs to subconsciously deceive the user into thinking they are talking to an actual human, unethical.</p>
<p>Social impact: emphasizes on the importance of transparency, informed consent, and respect for people.</p>
<p>Ex. VA imitates a deceased person to emotionally comfort the user is seen as unethical and may cause more harm than good.</p>

<h1>Care</h1>
<p>Interpretation: are able to affect relationships, question is whether it supports or damages those real human connections</p>
<p>Social impact:</p>
<ul>Positive: can be used to help assist the user that would normally be difficult or tedious with a person</ul>
<ul>Negative: brings into question if the agent is possible to provide genuine care like a human and replace it</ul>
<ul>Ex: Caregivers are able to use VAs to help assist in caring for elderly if they are swamped with other responsibilities, but shouldn’t and doesn’t entirely replace the need for that actual human assistance, companionship, and care.</ul>

<h1>Just (fairness)</h1>
<p>Interpretation: VAs are able to offer assistant to others, but there is the concern of it giving unequal benefits or harm to certain groups</p>
<p>Social impact: brings into question potential bias, unequal treatment, and limited accessibility in algorithms for underrepresented or marginalized groups.</p>
<p>Ex: Using AI assistants the will unintentionally filter out perfectly qualified applicants for a job position because they have a underrepresented background, this would be seen as unethical.</p>

<p>In order to reduce and mitigate the harms that come with virtual agents, there are several recommendations we can give. First it is important to recognize that virtual agents are only as good as the data that they are trained on, people looking to deploy this AI technology should recognize that proper and unbiased data is needed to avoid misinformation and mistakes later on. Secondly it is important to ensure privacy and security by being transparent about where the data is collected from and asking for individual consent concerning user data. To mitigate overreliance, human oversight should take precedence to ensure any mistakes or drawbacks that do occur are properly dealt with to avoid wrong decisions being made. Along with human oversight, virtual agents should not substitute for human interactions entirely and should only work with routine tasks that are repeatable that should in turn facilitate more human interactions for networking and rapport. Policy level action should also be taken by governmental bodies to ensure harms are mitigated. One policy level action can involve ensuring organizations or companies that use virtual agents be compliant with existing privacy laws and data protection laws. Transparency and bias/discrimination laws are also other actions that could be taken at the political level to ensure safety, trust and privacy.</p>

<br></br><br></br>References<br></br>
<a href="https://www.youtube.com/watch?v=-yBC9tyyczc">https://www.youtube.com/watch?v=-yBC9tyyczc</a>

<a href="https://ojs.aaai.org/index.php/AIES/article/view/31668/33835">https://ojs.aaai.org/index.php/AIES/article/view/31668/33835</a>

<a href="https://www.voxia.ai/blog/the-risks-of-ai-agents-and-how-we-can-solve-them-in-advance">https://www.voxia.ai/blog/the-risks-of-ai-agents-and-how-we-can-solve-them-in-advance</a>

<a href="https://www.destinationcrm.com/Articles/Columns-Departments/The-Tipping-Point/Why-AI-Agents-Are-Booming-and-the-Real-Challenges-Behind-the-Hype-169182.aspx">https://www.destinationcrm.com/Articles/Columns-Departments/The-Tipping-Point/Why-AI-Agents-Are-Booming-and-the-Real-Challenges-Behind-the-Hype-169182.aspx</a>

	

			</div></nav></body>
</html>
