<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>IT304 Project</title>
    <link rel="stylesheet" href="styles.css">
<style>
h1 {text-align: center;}
h2 {text-align: center;}
img {
  max-width: 100%;
  height: auto;
  border-radius: 40%;
}
</style>
</head>
<body>

    <nav>
<div class="topnav">
<ul>
    <li><a href="index1.html">Home</a></li>
    <li><a href="app.html">Applications</a></li>
    <a class="active" href="imp.html">Implications</a> 
    <li><a href="ref.html">References</a></li>
</ul>
</div>

	
              <h1>Implications</h1>
			  
	



<h2>Social Implications</h2>
<img src="AI_Social1.jpeg" alt="Social"> <br><br>
<p class="padding-10">
One of the social implications of virtual agents include that by relying on it too much, there can be a loss of opportunities of connecting, interacting, and building rapport with another person. An example of this is replacing a doctor in a doctor-patient relationship where there will be a loss of making a human connection with the doctor. 
</p><br><br>

<h2>Political / Social Concerns</h2>
<img src="AI_Social2.jpeg" alt="Political"><br><br>

<p class="padding-10">
The political and social concerns include privacy and data security. As virtual agents are integrated into various systems in different fields, concerns like data privacy, security, and misusage are being brought up, especially within ones that involve sensitive changing data. Another instance of privacy concern is that, currently, AI algorithms are being used in social media by, allegedly, listening in people’s conversation and searching through our search history to “tailor” the content we see to the user. There is also evidence of this claim when instances of virtual agent respond to the user even though the voice activation phrase wasn’t spoken and the command or question wasn’t directed towards it. Multiple news reports and articles, like KTNV 13 Las Vegas: Privacy threat? Smart speakers may be spying on you, have brought up this concern, informing the public of the potential dangers and is it really worth using AI. 
<br><br></p>

<p style="text-align: center;">		
<iframe width="560" height="315" src="https://www.youtube.com/embed/-yBC9tyyczc?si=lJnE1TW_NxZjLxQH" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe><br>





<br>
<h2>Equity and Access</h2>
<p class="padding-10">
There are also concerns of equity and access where not all fields and facilities have access to virtual agents to use since they aren’t all at the same level of development and funding. There could also be an over reliance and trust on virtual agents, assuming that they are always correct or helpful in any given situation and context. In the context of medical treatment, the agent could miss a detail and misdiagnose a patient. Here there is misplaced trust in the technology even though virtual agents are only as knowledgeable and experienced as it is limited to the depth of the data that it is programmed with.
</p>

<h2>Built-In Harms / Risk</h2>
<p class="padding-10">
One of the built-in harms and risks to using virtual agents include miscommunication. The agent could misunderstand or mishear the user’s words, leading to the wrong intended results. Another is placing too much trust in the technology and being too reliant on it. An example of this is doctors and hospitals over rely on virtual agents, leading to lower quality of care for the patients. Lastly, there is no concrete, proven benefit to using the technology. If the technology isn’t usable or helpful, then it can be a waste of time, resources, and could cause harm by replacing future better solutions.</p><br><br>

<h2>Ethical Lens</h2>
<img src="AI_Social.jpeg" alt="Ethical"><br><br>

<h2>Utilitarian</h2>
<p class="padding-10">
There are multiple ethical lenses that virtual agents can be seen through. One of them is the utilitarian (usefulness) lens. The interpretation is that virtual agents can be used to increase efficiency, reduce costs, and improve access in different fields (healthcare, customer service, education). This can be seen as ethical. The social impact is that it can be a positive usage for areas with marginalized populations since they will be able to use a new resource for assistance, but at the same time, there is the potential of virtual agents replacing the jobs of human workers in different fields. For example chat bots that offer mental support can help reduce stress and sucicide rates, but can replace the interaction/experience of going to a human therapist.</p>

<h2>Deontology</h2>
<p class="padding-10">
Another lens is deontology (duty-based). Its interpretation questions whether or not we should use it. Its concern is that virtual agents could subconsciously deceive the user into thinking they are talking to an actual human, where this is seen as unethical. The Social impact is that it emphasizes the importance of transparency, informed consent, and respect for people. For example, virtual agents imitating a deceased person to emotionally comfort the user is seen as unethical and may cause more harm than good.</p>

<h2>Care</h2>
<p class="padding-10">
The care len’s interpretation of using the agent is that it is able to affect relationships and whether it supports or damages real human connections. The social impact is that virtual agents can be used to help assist the user that would normally be difficult or tedious to, which is a positive. But using them brings into question if the agent is possible to provide genuine care like a human and replace it, which is seen as a negative. An example is that caregivers are able to use virtual agents to help assist in caring for elderly if they are swamped with other responsibilities, but shouldn’t and doesn’t entirely replace the need for that actual human assistance, companionship, and care.</p>

<h2>Just</h2>
<p class="padding-10">
Lastly, another lens is the just (fairness) lens. Its interpretation is that virtual agents are able to offer assistance to others, but there is the concern of it giving unequal benefits or harm to certain groups. The Social impact is that there could be the potential of bias, unequal treatment, and limited accessibility in algorithms for underrepresented or marginalized groups. An example of this is using AI assistants will unintentionally filter out perfectly qualified applicants for a job position because they have an underrepresented background or don’t hit those ‘key-words’ that the agent is programmed to favor, this would be seen as unethical.
</p><br><br>
<img src="AI_Reg.jpeg" alt="Regulation"><br><br>

<p class="padding-10">
In order to reduce and mitigate the harms that come with virtual agents, there are several recommendations we can give. First it is important to recognize that virtual agents are only as good as the data that they are trained on, people looking to deploy this AI technology should recognize that proper and unbiased data is needed to avoid misinformation and mistakes later on. 

<br><br>Secondly it is important to ensure privacy and security by being transparent about where the data is collected from and asking for individual consent concerning user data. To mitigate overreliance, human oversight should take precedence to ensure any mistakes or drawbacks that do occur are properly dealt with to avoid wrong decisions being made. Along with human oversight, virtual agents should not substitute for human interactions entirely and should only work with routine tasks that are repeatable that should in turn facilitate more human interactions for networking and rapport. 

<br><br>Policy level action should also be taken by governmental bodies to ensure harms are mitigated. One policy level action can involve ensuring organizations or companies that use virtual agents be compliant with existing privacy laws and data protection laws. Transparency and bias/discrimination laws are also other actions that could be taken at the political level to ensure safety, trust and privacy.</p><br><br>

<footer>
<p>IT304 - DL4: Final Project
<br><br>
Group 9: Ali Osman, Kesarah Touch, Kosal Touch, Werner Wyss Vargas<br></p>
</footer>

			</div></nav></body>
</html>
